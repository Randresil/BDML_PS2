ggplot(data = df) +
geom_point(aes(x = mpg, y = wt), color = "red", size = 2, shape = 0) +
geom_point(aes(x = mpg, y = wt_predicho), shape = 1) +
geom_line(aes(x = mpg, y = wt_predicho)) +
labs(x = "Eje X: Mile Per Gallon",
y = "Eje Y: Weight (lbs)",
title = "Titulo: MPG VS W",
subtitle = "Grafico de dispersión con valores predichos de regresión",
caption = "Datos de base de datos mtcars de R") +
xlim(min(df$mpg) - 1, max(df$mpg) + 1) +
ylim(min(df$wt) - 1, max(df$wt) + 1) +
theme_minimal()
df$cyl_2 = cyl^2
df$cyl_2 = df$cyl^2
df$cyl_3 = df$cyl^3
df <- df %>% mutate(wt_2 = wt^2)
economics
economics_long
dev.off()
df_econ <- economics
skim(df_econ)
summarise(df_econ)
summarize(df_econ)
desc(df_econ)
str(df_econ)
ggplot(df_econ) +
geom_line(aes(x = date, y = pce))
?economics
ggplot(df_econ) +
geom_line(aes(x = date, y = pce)) +
labs(x = "Year",
y = "Personal Consumption Expenditures (bill USD)",
title = "Evolution of PCE",
subtitle = "Time series of PCE",
caption = "Database retrieved from R")
ggplot(df_econ) +
geom_line(aes(x = date, y = pce)) +
labs(x = "Year",
y = "Personal Consumption Expenditures",
title = "Evolution of PCE",
subtitle = "Time series of PCE in billions of USD",
caption = "Database retrieved from R")
ggplot(df_econ) +
geom_line(aes(x = date, y = pop))
browseURL("https://www.geeksforgeeks.org/how-to-use-date-formats-in-r/", browser = getOption("Browser"))
browseURL("https://www.geeksforgeeks.org/how-to-use-date-formats-in-r/", browser = getOption(Browser))
browseURL("https://www.geeksforgeeks.org/how-to-use-date-formats-in-r/", browser = getOption("browser"))
df_time <- data.frame(date = as.Date("2022-01-01") + 0:730,
sales = runif(100, 20, 50))
df_time <- data.frame(date = as.Date("2022-01-01") + 0:730,
sales = runif(731, 20, 50))
head(df_time)
p_load(tidyverse, lubridate, skimr, stargazer, dplyr)
df_time$week <- floor_date(df_time$date, "week")
View(df_time)
View(df_time)
?floordate
?floor_date
df_time %>%
group_by(week) %>%
summarize(mean = mean(sales))
#Encontrando a nivel de mes
df_time$month <- floor_date(df$date, "month")
#Media de forma mensual
df_time %>%
group_by(month) %>%
summarize(mean = mean(sales))
df_time$month <- floor_date(df_time$date, "month")
df_time %>%
group_by(month) %>%
summarize(mean = mean(sales))
df_time2 <- df_time %>%
group_by(month) %>%
summarise(AverageSales = mean(sales))
class(date)
class(df_time$date)
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html"
library(pacman)
p_load(rvest,tidyverse, xml2, jsonlite)
html <- read_html(url)
table <- html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table()
table <- html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.tibble()
table <- html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
View(table)
browseURL("https://ignaciomsarmiento.github.io/GEIH2018_sample/", getOption("browser"))
?cat
?rbind
df2 <- rbind(table)
class(df2)
as.tibble(df2)
View(df2)
#===================================#
#### BIG DATA Y MACHINE LEARNING ####
#           PROBLEM SET #1          #
#===================================#
# Date: 02/09/2023
# R version 4.3.1
#==================================#
#### [1.] Paquetes y entorno ####
#==================================#
rm(list=ls())
install.packages("pacman")
library(pacman)
p_load(rvest, tidyverse, skimr, jsonlite)
install.packages("pacman")
for (i in 1:10) {
cat("Iteración = ", i, "\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
}
for (i in 1:10) {
cat("Iteración = ", i, "\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Xpath = "/html/body/table"
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
df_final <- rbind(df)
}
library(pacman)
p_load(rvest, tidyverse, skimr, jsonlite)
for (i in 1:10) {
cat("Iteración = ", i, "\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Xpath = "/html/body/table"
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
df_final <- rbind(df)
}
for (i in 1:2) {
cat("Iteración = ", i, "\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Clave: geih_html en network.
# Xpath = "/html/body/table"
# Creando tabla por web scraping
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
# Añadir df a dataframe final y remover df del loop
df_final <- rbind(df)
rm(df)
cat("-- Completada iteración --", "\n", "\n")
}
?bind_rows
for (i in 1:2) {
cat("-- Iteración = ", i, " --","\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Clave: geih_html en network.
# Xpath = "/html/body/table"
# Creando tabla por web scraping
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
# Añadir df a dataframe final y remover df del loop
df_final <- bind_rows(df)
rm(df)
cat("-- Completada iteración --", "\n", "\n")
}
rm(list=ls())
for (i in 1:2) {
rm(df)
cat("-- Iteración = ", i, " --","\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Clave: geih_html en network.
# Xpath = "/html/body/table"
# Creando tabla por web scraping
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
# Añadir df a dataframe final y remover df del loop
df_final <- bind_rows(df)
# Finalizada iteración del loop
cat("-- Completada iteración --", "\n", "\n")
}
?rbind
a <- rbind(a, df)
df_final <- data.frame()
df_final <- rbind(df_final, df)
df_final <- rbind(df_final, df)
df_final <- rbind(df_final, df)
df_final <- rbind(df_final, df)
ls()
df_final <- data.frame()
# For loop de web scraping
for (i in 1:2) {
cat("-- Iteración = ", i, " --","\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Clave: geih_html en network.
# Xpath = "/html/body/table"
# Creando tabla por web scraping
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
# Añadir df a dataframe final y remover df del loop
df_final <- rbind(df_final, df)
# Finalizada iteración del loop
cat("-- Completada iteración --", "\n", "\n")
}
# Definición de dataframe vacio
df_final <- data.frame()
# For loop de web scraping
for (i in 1:10) {
cat("-- Iteración = ", i, " --","\n")
url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_", i, ".html")
chunk_html <- read_html(url)
# Mismo xpath para cada uno de los data chunks de PAG WEB.
# Clave: geih_html en network.
# Xpath = "/html/body/table"
# Creando tabla por web scraping
df <- chunk_html %>%
html_nodes(xpath =  "/html/body/table") %>%
html_table() %>%
as.data.frame()
# Añadir df a dataframe final y remover df del loop
df_final <- rbind(df_final, df)
# Finalizada iteración del loop
cat("-- Completada iteración --", "\n", "\n")
}
object.size(df_final)
object.size(df_final, units = "Gb")
format(object.size(df_final), units = "Gb")
format(object.size(df_final), units = "GB")
format(object.size(df_final))
format(object.size(df_final), units = "Gb")
format(object.size(df_final), units = "GiB")
format(object.size(df_final), units = "b")
format(object.size(df_final), units = "mb")
format(object.size(df_final), units = "GB")
format(object.size(df_final), units = "MB")
?save
format(object.size(df_final), units = "MB")
write_csv(df_final, "df_final.csv")
write_csv(df_final, "/Users/ricardoandressilvatorres/Documents/GitHub/BigData-MachineLearning202320/Taller 1/df_final.csv")
View(df_final)
object.size(ls())
ls()
object.size(ls)
object.size(ls())
object.size(list = ls())
require("pacman")
p_load("tidyverse", "rio", "stargazer")
rm(list = ls())
auto <- import("https://www.stata-press.com/data/r17/auto.dta")
auto <- auto %>% mutate(mpg = as.numeric(mpg),
weight = as.numeric(weight),
foreign = as.numeric(foreign))
?ifelse
auto <- auto %>% mutate(mpg = foreign==1, mpg+8, mpg)
reg1 <- lm(mpg ~ foreign + weight, data = auto)
stargazer(reg1, type = "text", digits = 7)
auto <- import("https://www.stata-press.com/data/r17/auto.dta")
auto <- auto %>% mutate(mpg = as.numeric(mpg),
weight = as.numeric(weight),
foreign = as.numeric(foreign))
reg1 <- lm(mpg ~ foreign + weight, data = auto)
stargazer(reg1, type = "text", digits = 7)
auto <- auto %>% mutate(foreign==1, mpg+8, mpg)
reg1 <- lm(mpg ~ foreign + weight, data = auto)
reg1 <- lm(mpg ~ foreign + weight, data = auto)
stargazer(reg1, type = "text", digits = 7)
auto <- import("https://www.stata-press.com/data/r17/auto.dta")
auto <- auto %>% mutate(mpg = as.numeric(mpg),
weight = as.numeric(weight),
foreign = as.numeric(foreign))
reg1 <- lm(mpg ~ foreign + weight, data = auto)
stargazer(reg1, type = "text", digits = 7)
auto <- auto %>% mutate(foreign==1, mpg+8, mpg)
reg1 <- lm(mpg ~ foreign + weight, data = auto)
stargazer(reg1, type = "text", digits = 7)
auto <- auto %>% mutate(weightResidF = lm(weight ~ foreign, auto)$residuals)
auto <- auto %>% mutate(mpgResidF = lm(mpg ~ foreign, auto)$residuals)
reg2 <- lm(mpgResidF ~ weightResidF, auto)
stargazer(reg1, reg2, type = "text", digits = 7)
sum(resid(reg1)^2)
sum(resid(reg2)^2)
sqrt(diag(vcov(reg2))*(72/71))[2]
sqrt(diag(vcov(reg1)))[3]
r1<-lm(mpg ~ foreign + weight,auto)
r2<-lm(mpg~weight,auto)
stargazer(r1,r2,type="text",digits=7)
?with
with(auto,cor(foreign,weight))
cor(foreign,weight)
?cor
ggplot(auto,aes(y=mpg, x=weight, group=foreign, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight, group=foreign, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=TRUE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight, group=foreign, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight, group=foreign)) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight,  group=foreign, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
geom_abline(slope=r2$coefficients[2],
intercept=r2$coefficients[1],
color="orange", size=1) +
theme_bw()
ggplot(auto,aes(y=mpg, x=weight,  group=foreign, col=factor(foreign))) +
geom_point() +
geom_smooth(method = lm,se=FALSE)+
theme_bw()
auto<- auto %>% mutate(new_weight=weight+1000*foreign) #k = 1000
r3<-lm(mpg~new_weight+foreign,auto)
stargazer(r1,r3,type="text",digits=7)
r4<-lm(mpg~foreign+weightResidF,auto)
stargazer(r4,type="text",digits=7)
with(auto,cor(weightResidF,foreign))
r5<-lm(mpg~weightResidF,auto)
stargazer(r4,r5,type="text",digits=7)
ggplot(auto,aes(y=mpg, x=weightResidF ,group=foreign ,col=factor(foreign))) +
geom_point() +
geom_abline(slope=r4$coefficients[3],
intercept=r4$coefficients[1],
color="red", size=1) +
geom_abline(slope=r4$coefficients[3],
intercept=r4$coefficients[1]+r4$coefficients[2],
color="blue", size=1) +
geom_abline(slope=r5$coefficients[2],
intercept=r5$coefficients[1],
color="darkgreen", size=1) +
theme_bw()
r6<-lm(mpgResidF~weightResidF+foreign,auto)
stargazer(r4,r5,r6,type="text",digits=7)
r7<-lm(mpgResidF~weightResidF,auto)
stargazer(r6,r7,type="text",digits=7)
ggplot(auto , aes(y=mpgResidF, x=weightResidF, group=foreign, col=factor(foreign))) +
geom_point() +
geom_abline(slope=r6$coefficients[2],
intercept=r6$coefficients[1],
color="red", size=1) +
geom_abline(slope=r7$coefficients[2],
intercept=r7$coefficients[1],
color="darkgreen", size=1) +
theme_bw()
browseURL("https://towardsdatascience.com/the-fwl-theorem-or-how-to-make-all-regressions-intuitive-59f801eb3299")
library(pacman)
p_load(tidyverse,  # Manipulacio de dataframes
sf,         # Leer, escribir y manipular datos espaciales
tmaptools,  # Conexcion de API de OSM
osmdata,    # Obtener datos de Open Street Map (OSM)
leaflet,    # Mapas interactivos
tidymodels, # Para modelos de ML
rio,        # Importacion
stargazer,  # Tablas bonitas de regs y estad desc
rgeos, # Calcular centroides de un poligono
skimr,
glmnet,
randomForest, # Modelos de bosque aleatorio
rattle, # Interfaz gráfica para el modelado de datos
spatialsample) # Muestreo espacial para modelos de aprendizaje automático)
getwd()
directorio <- "/Users/ricardoandressilvatorres/Documents/GitHub/BDML_PS2"
setwd(directorio)
install_formats() # Cuestiones de importacion de archivos del paquete rio
list.files()
list.files("Stores/")
data_tot <- import("Stores/Data_total.csv")
train <- data_tot %>% filter(div == "train")
test <- data_tot %>% filter(div == "test")
df_fold <- vfold_cv(train, v = 5)
# RANDOM FOREST
train_sf <- st_as_sf(train, coords = c("lon", "lat"), crs=4326)
set.seed(123)
block_folds <- spatial_block_cv(train_sf, v = 5)
recipe <- recipe(formula = price ~ distancia_park + distancia_stadium + bank + bus_station + college + hospital +
police + university + pub + veterinary + mall + nature_reserve + parqueadero + terraza + piscina +
conjunto + apartaestudio + duplex + vista + penthouse + casa + habitaciones_numerico + bano_numerico +
metros_num, data = train) %>%
step_poly(metros_num, mall = 2) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
# Tune grid aleatorio para el modelo de rf
rf_grid_random <- grid_random(  mtry(range = c(6, 14)),
min_n(range = c(4, 14)),
trees(range = c(100, 120)), size = 4)
## Modelo de rf
rf_spec<- rand_forest(
mtry = tune(),              # Hiperparámetro: Número de variables a considerar en cada división
min_n = tune(),             # Hiperparámetro: Profundidad mínima del árbol
trees = tune(),
) %>%
set_engine("randomForest") %>%
set_mode("regression")       # Cambiar a modo de regresión
workflow_rf <- workflow() %>%
add_recipe(recipe) %>%
add_model(rf_spec)
tune_rf <- tune_grid(
workflow_rf,
resamples = block_folds,
grid = rf_grid_random,
metrics = metric_set(mae))
best_parms_rf<- select_best(tune_rf, metric = "mae")
recipe <- recipe(formula = price ~ distancia_park + distancia_stadium + bank + bus_station + college + hospital +
police + university + pub + veterinary + mall + nature_reserve + parqueadero + terraza + piscina +
conjunto + apartaestudio + duplex + vista + penthouse + casa + habitaciones_numerico + bano_numerico +
metros_num, data = train) %>%
step_poly(metros_num = 2) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
# Tune grid aleatorio para el modelo de rf
rf_grid_random <- grid_random(  mtry(range = c(6, 14)),
min_n(range = c(4, 14)),
trees(range = c(100, 120)), size = 4)
## Modelo de rf
rf_spec<- rand_forest(
mtry = tune(),              # Hiperparámetro: Número de variables a considerar en cada división
min_n = tune(),             # Hiperparámetro: Profundidad mínima del árbol
trees = tune(),
) %>%
set_engine("randomForest") %>%
set_mode("regression")       # Cambiar a modo de regresión
workflow_rf <- workflow() %>%
add_recipe(recipe) %>%
add_model(rf_spec)
tune_rf <- tune_grid(
workflow_rf,
resamples = block_folds,
grid = rf_grid_random,
metrics = metric_set(mae))
recipe <- recipe(formula = price ~ distancia_park + distancia_stadium + bank + bus_station + college + hospital +
police + university + pub + veterinary + mall + nature_reserve + parqueadero + terraza + piscina +
conjunto + apartaestudio + duplex + vista + penthouse + casa + habitaciones_numerico + bano_numerico +
metros_num, data = train) %>%
# step_poly(metros_num = 2)
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
# Tune grid aleatorio para el modelo de rf
rf_grid_random <- grid_random(  mtry(range = c(6, 14)),
min_n(range = c(4, 14)),
trees(range = c(100, 120)), size = 4)
## Modelo de rf
rf_spec<- rand_forest(
mtry = tune(),              # Hiperparámetro: Número de variables a considerar en cada división
min_n = tune(),             # Hiperparámetro: Profundidad mínima del árbol
trees = tune(),
) %>%
set_engine("randomForest") %>%
set_mode("regression")       # Cambiar a modo de regresión
workflow_rf <- workflow() %>%
add_recipe(recipe) %>%
add_model(rf_spec)
tune_rf <- tune_grid(
workflow_rf,
resamples = block_folds,
grid = rf_grid_random,
metrics = metric_set(mae))
best_parms_rf<- select_best(tune_rf, metric = "mae")
best_parms_rf
rf_final <- finalize_workflow(workflow_rf, best_parms_rf)
rf_final_fit <- fit(rf_final, data = train)
prediccion_rf <- predict(rf_final_fit, new_data = test)
prediccion_rf
prediccion_rf <- test %>% select(property_id) %>% bind_cols(prediccion_rf) %>% rename(price = .pred, property_id = property_id)
write.csv(prediccion_rf, file = 'Stores/prediccion_rf2.csv', row.names = FALSE)
